{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T13:25:09.743660Z",
     "iopub.status.busy": "2025-02-09T13:25:09.743311Z",
     "iopub.status.idle": "2025-02-09T13:25:13.021878Z",
     "shell.execute_reply": "2025-02-09T13:25:13.021082Z",
     "shell.execute_reply.started": "2025-02-09T13:25:09.743627Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ! git clone https://github.com/t1hachane/nckh_2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T13:25:13.023711Z",
     "iopub.status.busy": "2025-02-09T13:25:13.023447Z",
     "iopub.status.idle": "2025-02-09T13:25:13.028958Z",
     "shell.execute_reply": "2025-02-09T13:25:13.027982Z",
     "shell.execute_reply.started": "2025-02-09T13:25:13.023682Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/diepht/ha_xai/nckh_2025/xai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diepht/ha_xai/myenv/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd 'xai'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T13:25:13.030799Z",
     "iopub.status.busy": "2025-02-09T13:25:13.030515Z",
     "iopub.status.idle": "2025-02-09T13:25:14.255329Z",
     "shell.execute_reply": "2025-02-09T13:25:14.254215Z",
     "shell.execute_reply.started": "2025-02-09T13:25:13.030774Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ! git pull origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T13:25:14.258039Z",
     "iopub.status.busy": "2025-02-09T13:25:14.257657Z",
     "iopub.status.idle": "2025-02-09T13:25:14.262594Z",
     "shell.execute_reply": "2025-02-09T13:25:14.261647Z",
     "shell.execute_reply.started": "2025-02-09T13:25:14.257991Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_name = 'tcga-gbm-methxgexcnv-2000-3-omics'\n",
    "COHORT = 'TCGA_GBM_GExCNVxMETH_2000_MinMaxScaler'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T13:25:14.264506Z",
     "iopub.status.busy": "2025-02-09T13:25:14.264113Z",
     "iopub.status.idle": "2025-02-09T13:25:19.860717Z",
     "shell.execute_reply": "2025-02-09T13:25:19.859467Z",
     "shell.execute_reply.started": "2025-02-09T13:25:14.264465Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "!pip install watermark --quiet\n",
    "import random\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T13:25:19.862549Z",
     "iopub.status.busy": "2025-02-09T13:25:19.862264Z",
     "iopub.status.idle": "2025-02-09T13:25:19.868778Z",
     "shell.execute_reply": "2025-02-09T13:25:19.867893Z",
     "shell.execute_reply.started": "2025-02-09T13:25:19.862517Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diepht/ha_xai/myenv/lib/python3.12/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/kaggle/working/nckh_2025/xai'\n",
      "/home/diepht/ha_xai/nckh_2025\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# tmp = f'/kaggle/input/{dataset_name}'\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# %cd {tmp}\u001b[39;00m\n\u001b[1;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/working/nckh_2025/xai\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_model_dict\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_model_dict\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtrain_test\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m prepare_trte_data, gen_trte_adj_mat, test_epoch\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "# tmp = f'/kaggle/input/{dataset_name}'\n",
    "# %cd {tmp}\n",
    "%cd '/kaggle/working/nckh_2025/xai'\n",
    "from models import init_model_dict\n",
    "from utils import load_model_dict\n",
    "from train_test import prepare_trte_data, gen_trte_adj_mat, test_epoch\n",
    "\n",
    "from train_test import train_test\n",
    "from train_test import gen_trte_adj_mat\n",
    "from utils import save_model_dict\n",
    "\n",
    "# %cd '/kaggle/working'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T13:25:19.870331Z",
     "iopub.status.busy": "2025-02-09T13:25:19.870048Z",
     "iopub.status.idle": "2025-02-09T13:25:25.452727Z",
     "shell.execute_reply": "2025-02-09T13:25:25.451648Z",
     "shell.execute_reply.started": "2025-02-09T13:25:19.870305Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "from datetime import datetime\n",
    "\n",
    "!pip install captum --quiet\n",
    "from captum.attr import IntegratedGradients, GradientShap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T13:25:25.454721Z",
     "iopub.status.busy": "2025-02-09T13:25:25.454419Z",
     "iopub.status.idle": "2025-02-09T13:25:25.460525Z",
     "shell.execute_reply": "2025-02-09T13:25:25.459469Z",
     "shell.execute_reply.started": "2025-02-09T13:25:25.454690Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import copy\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from IPython.display import Markdown\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T13:25:25.463968Z",
     "iopub.status.busy": "2025-02-09T13:25:25.463720Z",
     "iopub.status.idle": "2025-02-09T13:25:25.516534Z",
     "shell.execute_reply": "2025-02-09T13:25:25.515563Z",
     "shell.execute_reply.started": "2025-02-09T13:25:25.463944Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda: True\n",
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "Author: Le Hoang\n",
      "\n",
      "Last updated: 2025-02-09\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.7.10\n",
      "IPython version      : 7.22.0\n",
      "\n",
      "torch: 1.7.0\n",
      "numpy: 1.19.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'cuda: {cuda}')\n",
    "%load_ext watermark\n",
    "%watermark -a 'Le Hoang' -u -d -v -p torch,numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T13:25:25.519202Z",
     "iopub.status.busy": "2025-02-09T13:25:25.518846Z",
     "iopub.status.idle": "2025-02-09T13:25:25.525545Z",
     "shell.execute_reply": "2025-02-09T13:25:25.524582Z",
     "shell.execute_reply.started": "2025-02-09T13:25:25.519160Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "\n",
    "# Config\n",
    "rseed = 42\n",
    "set_seed(rseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T13:25:25.527251Z",
     "iopub.status.busy": "2025-02-09T13:25:25.526905Z",
     "iopub.status.idle": "2025-02-09T13:25:25.535857Z",
     "shell.execute_reply": "2025-02-09T13:25:25.535067Z",
     "shell.execute_reply.started": "2025-02-09T13:25:25.527216Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "postfix_tr = '_tr'\n",
    "postfix_te = '_val'\n",
    "\n",
    "data_folder = f'/kaggle/input/{dataset_name}/{COHORT}'\n",
    "model_folder = '/kaggle/working/models'\n",
    "train_file = f'/kaggle/working/nckh_2025/xai/main_mmd.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T13:25:25.537204Z",
     "iopub.status.busy": "2025-02-09T13:25:25.536936Z",
     "iopub.status.idle": "2025-02-09T13:25:25.549424Z",
     "shell.execute_reply": "2025-02-09T13:25:25.548716Z",
     "shell.execute_reply.started": "2025-02-09T13:25:25.537171Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Subtypes\n",
    "loc_file_json_id_omic = data_folder + '/1/dct_index_subtype.json'\n",
    "with open(loc_file_json_id_omic) as file_json_id_omic:\n",
    "    dct_LABEL_MAPPING_NAME = json.load(file_json_id_omic)\n",
    "    # dct_LABEL_MAPPING_NAME = {int(k): v for k,v in dct_LABEL_MAPPING_NAME.items()} # convert str number key to int\n",
    "LABEL_MAPPING_NAME = dct_LABEL_MAPPING_NAME.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T13:25:25.550965Z",
     "iopub.status.busy": "2025-02-09T13:25:25.550627Z",
     "iopub.status.idle": "2025-02-09T13:25:25.557117Z",
     "shell.execute_reply": "2025-02-09T13:25:25.556357Z",
     "shell.execute_reply.started": "2025-02-09T13:25:25.550930Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_models=4\n",
    "idx_list = list(range(1,num_models+1))\n",
    "view_list = [1,2,3]\n",
    "\n",
    "# Hyperparameters\n",
    "num_epoch= 1500\n",
    "# lr_e = 1e-4\n",
    "lr = 1e-4\n",
    "# dim_he_list=[250,300,150]\n",
    "hidden_dim = [1000]\n",
    "patience=200\n",
    "verbose = False\n",
    "\n",
    "retrain=True\n",
    "bool_using_early_stopping = True\n",
    "verbose = False\n",
    "print_hyper = True\n",
    "Run_MMD = True\n",
    "testonly = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T13:25:25.558843Z",
     "iopub.status.busy": "2025-02-09T13:25:25.558458Z",
     "iopub.status.idle": "2025-02-09T13:25:26.607008Z",
     "shell.execute_reply": "2025-02-09T13:25:26.606029Z",
     "shell.execute_reply.started": "2025-02-09T13:25:25.558797Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Count per Subtypes: \n",
      "\n",
      "Train set\n",
      " Classical  Mesenchymal  Neural  Proneural\n",
      "        43           48      28         43 \n",
      "\n",
      "Test set\n",
      " Classical  Mesenchymal  Neural  Proneural\n",
      "        14           16       9         15 \n",
      "\n",
      "Validation set\n",
      " Classical  Mesenchymal  Neural  Proneural\n",
      "        14           17       9         14 \n",
      "\n",
      "\n",
      "Count Samples: \n",
      "\n",
      " Train set  Test set  Validation set\n",
      "       162        54              54 \n",
      "\n",
      " Train set  Test set  Validation set\n",
      "       162        54              54 \n",
      "\n",
      " Train set  Test set  Validation set\n",
      "       162        54              54 \n",
      "\n",
      "\n",
      "Count Features: \n",
      "\n",
      " Omic 1  Omic 2  Omic 3\n",
      "   2000    2000    2000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check data size\n",
    "for fold_id in [3]:\n",
    "#     print(f'idx data: {fold_id}')\n",
    "    tmp = list(LABEL_MAPPING_NAME)\n",
    "    label_files = ['tr', 'te', 'val']\n",
    "    dict = {\n",
    "        'tr': 'Train set',\n",
    "        'te': 'Test set',\n",
    "        'val': 'Validation set'\n",
    "    }\n",
    "    \n",
    "    print('\\nCount per Subtypes: \\n')\n",
    "    for label_file in label_files:\n",
    "        df = pd.read_csv(f'{data_folder}/{fold_id}/labels_{label_file}.csv', header=None, names=['subtypes'])\n",
    "        subtype_counts = df['subtypes'].value_counts().sort_index()\n",
    "        \n",
    "        print(f'{dict[label_file]}')\n",
    "        \n",
    "        res = {}\n",
    "        for subtype, count in subtype_counts.items():\n",
    "            res[tmp[subtype]] = count\n",
    "\n",
    "        print(pd.DataFrame(res, index=[0]).to_string(index=False), '\\n')\n",
    "    \n",
    "    print('\\nCount Samples: \\n')\n",
    "    for idx in view_list:\n",
    "        res = {}\n",
    "        for label_file in label_files:\n",
    "            df = pd.read_csv(f'{data_folder}/{fold_id}/{idx}_{label_file}.csv', header=None)\n",
    "            res[dict[label_file]] = df.shape[0]\n",
    "            \n",
    "        print(pd.DataFrame(res, index=[0]).to_string(index=False), '\\n')\n",
    "    \n",
    "    print('\\nCount Features: \\n')\n",
    "    res={}\n",
    "    for idx in view_list:\n",
    "        df = pd.read_csv(f'{data_folder}/{fold_id}/{idx}_featname.csv', header=None, names=['featname'])\n",
    "        res[f\"Omic {idx}\"] = df.shape[0]\n",
    "        \n",
    "    print(pd.DataFrame(res, index=[0]).to_string(index=False), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T13:25:26.608713Z",
     "iopub.status.busy": "2025-02-09T13:25:26.608393Z",
     "iopub.status.idle": "2025-02-09T13:25:26.613322Z",
     "shell.execute_reply": "2025-02-09T13:25:26.612358Z",
     "shell.execute_reply.started": "2025-02-09T13:25:26.608683Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_view = len(view_list)\n",
    "num_class = len(LABEL_MAPPING_NAME)\n",
    "if num_class == 2:\n",
    "    adj_parameter = 2\n",
    "    dim_he_list = [200,200,100]\n",
    "if num_class > 2:\n",
    "    adj_parameter = 10\n",
    "#     dim_he_list = [400,400,200]\n",
    "dim_hvcdn= pow(num_class,num_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T13:25:26.614714Z",
     "iopub.status.busy": "2025-02-09T13:25:26.614440Z",
     "iopub.status.idle": "2025-02-09T13:25:26.627960Z",
     "shell.execute_reply": "2025-02-09T13:25:26.626856Z",
     "shell.execute_reply.started": "2025-02-09T13:25:26.614688Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "added_softmax = False\n",
    "\n",
    "# Prepairing raw data\n",
    "def preprocessing_data(tup_tensor_test_data, data_folder):\n",
    "    data_tr_list = []\n",
    "    data_te_list = []\n",
    "#     print(view_list)\n",
    "\n",
    "    for i in view_list:\n",
    "        data_tr_list.append(torch.tensor(np.loadtxt(os.path.join(data_folder, str(i)+\"_tr.csv\"), delimiter=','),dtype=torch.float32))\n",
    "        data_te_list.append(tup_tensor_test_data[i-1])\n",
    "        if cuda:\n",
    "            data_tr_list[i-1] = data_tr_list[i-1].to(device)\n",
    "            data_te_list[i-1] = data_te_list[i-1].to(device)       \n",
    "\n",
    "    # num train's records, test's records\n",
    "    num_tr = data_tr_list[0].shape[0]\n",
    "    num_te = data_te_list[0].shape[0]\n",
    "\n",
    "    # idx\n",
    "    trte_idx = {}\n",
    "    trte_idx[\"tr\"] = list(range(num_tr))\n",
    "    trte_idx[\"te\"] = list(range(num_tr, (num_tr+num_te)))\n",
    "\n",
    "    # num of views or num of omics\n",
    "    num_view = len(view_list)\n",
    "    data_tensor_list = []\n",
    "    for i in range(num_view):\n",
    "        data_tensor_list.append(torch.cat((data_tr_list[i], data_te_list[i]), axis=0))\n",
    "        if cuda:\n",
    "            data_tensor_list[i] = data_tensor_list[i].to(device)#cuda()\n",
    "    \n",
    "    data_train_list = []\n",
    "    data_trte_list = []\n",
    "    for i in range(len(data_tensor_list)):\n",
    "        data_train_list.append(data_tensor_list[i][trte_idx[\"tr\"]].clone())\n",
    "\n",
    "        tup_seq_data = (data_tensor_list[i][trte_idx[\"tr\"]].clone(), data_tensor_list[i][trte_idx[\"te\"]].clone())\n",
    "        data_trte_list.append(\n",
    "            torch.cat(tup_seq_data,axis=0)\n",
    "        )\n",
    "    return data_train_list, data_trte_list,trte_idx\n",
    "\n",
    "\n",
    "# III. For Feature Important\n",
    "\n",
    "def custom_logit_predictor(*tup_tensor_data, data_folder):\n",
    "    # Set softmax flag\n",
    "    global added_softmax\n",
    "    added_softmax = True\n",
    "\n",
    "    # Move model to device\n",
    "    if cuda:\n",
    "        model_dict['MMDynamic'].to(device)\n",
    "    model_dict['MMDynamic'].eval()\n",
    "\n",
    "    # Process input tensors\n",
    "    tup_tensor_data = tuple(tensor_data.to(device) if cuda else tensor_data \n",
    "                           for tensor_data in tup_tensor_data)\n",
    "\n",
    "    # Preprocess data\n",
    "    data_tr_list, data_trte_list, trte_idx = preprocessing_data(tup_tensor_data, data_folder)\n",
    "\n",
    "    # Get predictions using MMDynamic model\n",
    "    with torch.set_grad_enabled(True):\n",
    "        predictions = model_dict['MMDynamic'].infer(data_trte_list)\n",
    "\n",
    "    # Get test predictions\n",
    "    predictions = predictions[trte_idx[\"te\"],:]\n",
    "\n",
    "    # Apply softmax\n",
    "    if added_softmax:\n",
    "        predictions = F.softmax(predictions, dim=1)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T13:25:26.629533Z",
     "iopub.status.busy": "2025-02-09T13:25:26.629230Z",
     "iopub.status.idle": "2025-02-09T13:25:26.642788Z",
     "shell.execute_reply": "2025-02-09T13:25:26.641833Z",
     "shell.execute_reply.started": "2025-02-09T13:25:26.629503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_dict=None\n",
    "def load_model(data_folder, model_folder):\n",
    "    # ---- load paper's model1\n",
    "    data_tr_list, data_trte_list, trte_idx, labels_trte = prepare_trte_data(data_folder, view_list, postfix_tr='_tr', postfix_te='_val')\n",
    "    dim_list = [x.shape[1] for x in data_tr_list]\n",
    "\n",
    "    global model_dict\n",
    "    model_dict = init_model_dict(num_class, dim_list, hidden_dim)\n",
    "\n",
    "    model_dict = load_model_dict(model_folder, model_dict)\n",
    "    # ---- Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T13:25:26.644415Z",
     "iopub.status.busy": "2025-02-09T13:25:26.644136Z",
     "iopub.status.idle": "2025-02-09T13:25:26.656521Z",
     "shell.execute_reply": "2025-02-09T13:25:26.655516Z",
     "shell.execute_reply.started": "2025-02-09T13:25:26.644387Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def display_classification_report(\n",
    "    n_class,\n",
    "    conf_matrix,\n",
    "    avg_report,\n",
    "    label_mapping_name,\n",
    "    cmap=\"Blues\",\n",
    "    fmt=\".2%\",\n",
    "    annot=True,\n",
    "    path=None,  # str path to save fig. If not None\n",
    "    shown=True,\n",
    "):\n",
    "    clf_df = avg_report\n",
    "    clf_df.loc[[\"precision\", \"recall\"], \"accuracy\"] = np.nan\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_figwidth(12)\n",
    "    \n",
    "    ConfusionMatrixDisplay(conf_matrix, display_labels=label_mapping_name).plot(cmap=cmap, ax=ax1)\n",
    "    \n",
    "    sns.heatmap(clf_df.iloc[:-1, :].T, annot=annot, cmap=cmap, robust=True, ax=ax2, fmt=fmt)\n",
    "    \n",
    "    if path is not None:\n",
    "        fig.savefig(path, dpi=300)\n",
    "    if shown:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T13:25:26.658234Z",
     "iopub.status.busy": "2025-02-09T13:25:26.657971Z",
     "iopub.status.idle": "2025-02-09T13:25:26.670473Z",
     "shell.execute_reply": "2025-02-09T13:25:26.669507Z",
     "shell.execute_reply.started": "2025-02-09T13:25:26.658210Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_average_report(reports):\n",
    "    \"\"\"Calculate the average classification report from a list of reports.\"\"\"\n",
    "    avg_report = pd.DataFrame(reports[0]).copy()\n",
    "    for report in reports[1:]:\n",
    "        avg_report += pd.DataFrame(report)\n",
    "    avg_report /= len(reports)\n",
    "    return avg_report\n",
    "\n",
    "def evaluate_model(bool_report=True, _type_data='te'):\n",
    "    conf_matrix = np.zeros((num_class, num_class), dtype=int)\n",
    "\n",
    "    reports = []\n",
    "    results = []\n",
    "\n",
    "    for idx in idx_list:\n",
    "        cur_model_folder = f'{model_folder}/{idx}'\n",
    "        cur_data_folder = f\"{data_folder}/{idx}/\"\n",
    "        load_model(cur_data_folder, cur_model_folder)\n",
    "\n",
    "        _data_list = []\n",
    "\n",
    "        _label = np.loadtxt(os.path.join(cur_data_folder, f\"labels_{_type_data}.csv\"), delimiter=',').astype(int)\n",
    "\n",
    "        for i in view_list:\n",
    "            _data_loc = os.path.join(cur_data_folder, f\"{i}_{_type_data}.csv\")\n",
    "            _data_list.append(np.loadtxt(_data_loc, delimiter=','))\n",
    "        \n",
    "        _tensor_data_list = tuple(torch.tensor(np_arr, dtype=torch.float32).to(device) for np_arr in _data_list)\n",
    "        pred = custom_logit_predictor(*_tensor_data_list, data_folder=cur_data_folder)\n",
    "        pred = np.array(torch.argmax(pred.cpu(), dim=1))\n",
    "\n",
    "        fold_conf_matrix = confusion_matrix(_label, pred, labels=np.arange(num_class))\n",
    "        conf_matrix += fold_conf_matrix\n",
    "        \n",
    "        acc = accuracy_score(_label, pred)\n",
    "        f1_macro = f1_score(_label, pred, average='macro')\n",
    "        f1_weighted = f1_score(_label, pred, average='weighted')\n",
    "        results.append((idx, acc, f1_macro, f1_weighted))\n",
    "\n",
    "        # Get classification report for the current fold\n",
    "        report = classification_report(_label, pred, target_names=LABEL_MAPPING_NAME, output_dict=True)\n",
    "        reports.append(report)\n",
    "\n",
    "    if bool_report:\n",
    "        if not os.path.exists(\"/kaggle/working/phase1\"):\n",
    "            os.makedirs(\"/kaggle/working/phase1\")\n",
    "        \n",
    "        # Calculate average classification report\n",
    "        avg_report = calculate_average_report(reports)\n",
    "        \n",
    "        # Calculate the average of the models\n",
    "        df = pd.DataFrame(results, columns=['Model', 'Accuracy', 'F1 Macro', 'F1 Weighted'])\n",
    "        avg_row = ['Average', df['Accuracy'].mean(), df['F1 Macro'].mean(), df['F1 Weighted'].mean()]\n",
    "        df.loc[len(df)] = avg_row\n",
    "\n",
    "        print(df.to_string(index=False))\n",
    "\n",
    "        # Display confusion matrix and classification report\n",
    "        display_classification_report(\n",
    "            num_class,\n",
    "            conf_matrix,\n",
    "            avg_report,\n",
    "            LABEL_MAPPING_NAME,\n",
    "            path=f\"/kaggle/working/phase1/Evaluate_model_{_type_data}\",\n",
    "        )\n",
    "\n",
    "    return avg_report['macro avg']['f1-score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1. TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T13:25:26.671868Z",
     "iopub.status.busy": "2025-02-09T13:25:26.671540Z",
     "iopub.status.idle": "2025-02-09T13:26:15.446242Z",
     "shell.execute_reply": "2025-02-09T13:26:15.445210Z",
     "shell.execute_reply.started": "2025-02-09T13:25:26.671838Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx data: 1\n",
      "200\n",
      "\n",
      "            Config:\n",
      "                * Reproduce:\n",
      "                - Random Seed\n",
      "                    = 42\n",
      "\n",
      "                * Data\n",
      "                - Data Folder\n",
      "                    = /kaggle/input/tcga-gbm-methxgexcnv-2000-3-omics/TCGA_GBM_GExCNVxMETH_2000_MinMaxScaler/1\n",
      "                - Data Train\n",
      "                    = _tr\n",
      "                - Data Test\n",
      "                    = _val\n",
      "                - Saved Model Loc\n",
      "                    = /kaggle/working/models/1\n",
      "                - List Views\n",
      "                    = [1, 2, 3]\n",
      "\n",
      "                *\n",
      "                - Num Epoch PostTrain\n",
      "                    = 1500\n",
      "\n",
      "                - Lr Classifier\n",
      "                    = 0.0001\n",
      "                - Hidden dim\n",
      "                    = [1000]\n",
      "\n",
      "                *\n",
      "                - Test Only\n",
      "                    = False\n",
      "\n",
      "                - Patience\n",
      "                    = 200\n",
      "                - Verbose   \n",
      "                    = False\n",
      "            \n",
      "\n",
      "Training...\n",
      "\n",
      "Test: Epoch 0\n",
      "Test F1 weighted: 0.16270\n",
      "Test F1 macro: 0.16071\n",
      "\n",
      "Test: Epoch 50\n",
      "Test F1 weighted: 0.58701\n",
      "Test F1 macro: 0.54113\n",
      "\n",
      "Test: Epoch 100\n",
      "Test F1 weighted: 0.71010\n",
      "Test F1 macro: 0.66743\n",
      "\n",
      "Test: Epoch 150\n",
      "Test F1 weighted: 0.83263\n",
      "Test F1 macro: 0.83315\n",
      "\n",
      "Test: Epoch 200\n",
      "Test F1 weighted: 0.85089\n",
      "Test F1 macro: 0.84938\n",
      "\n",
      "Test: Epoch 250\n",
      "Test F1 weighted: 0.86920\n",
      "Test F1 macro: 0.86770\n",
      "\n",
      "Test: Epoch 300\n",
      "Test F1 weighted: 0.86920\n",
      "Test F1 macro: 0.86770\n",
      "\n",
      "Test: Epoch 350\n",
      "Test F1 weighted: 0.86920\n",
      "Test F1 macro: 0.86770\n",
      "\n",
      "Test: Epoch 400\n",
      "Test F1 weighted: 0.86920\n",
      "Test F1 macro: 0.86770\n",
      "Early stop at epoch 415th after 200 epochs not increasing score from epoch 215th with best score 0.8895451965375337\n",
      "****************************************************************************************************\n",
      "idx data: 2\n",
      "200\n",
      "\n",
      "            Config:\n",
      "                * Reproduce:\n",
      "                - Random Seed\n",
      "                    = 42\n",
      "\n",
      "                * Data\n",
      "                - Data Folder\n",
      "                    = /kaggle/input/tcga-gbm-methxgexcnv-2000-3-omics/TCGA_GBM_GExCNVxMETH_2000_MinMaxScaler/2\n",
      "                - Data Train\n",
      "                    = _tr\n",
      "                - Data Test\n",
      "                    = _val\n",
      "                - Saved Model Loc\n",
      "                    = /kaggle/working/models/2\n",
      "                - List Views\n",
      "                    = [1, 2, 3]\n",
      "\n",
      "                *\n",
      "                - Num Epoch PostTrain\n",
      "                    = 1500\n",
      "\n",
      "                - Lr Classifier\n",
      "                    = 0.0001\n",
      "                - Hidden dim\n",
      "                    = [1000]\n",
      "\n",
      "                *\n",
      "                - Test Only\n",
      "                    = False\n",
      "\n",
      "                - Patience\n",
      "                    = 200\n",
      "                - Verbose   \n",
      "                    = False\n",
      "            \n",
      "\n",
      "Training...\n",
      "\n",
      "Test: Epoch 0\n",
      "Test F1 weighted: 0.15676\n",
      "Test F1 macro: 0.16444\n",
      "\n",
      "Test: Epoch 50\n",
      "Test F1 weighted: 0.66125\n",
      "Test F1 macro: 0.59602\n",
      "\n",
      "Test: Epoch 100\n",
      "Test F1 weighted: 0.77817\n",
      "Test F1 macro: 0.74994\n",
      "\n",
      "Test: Epoch 150\n",
      "Test F1 weighted: 0.82980\n",
      "Test F1 macro: 0.81831\n",
      "\n",
      "Test: Epoch 200\n",
      "Test F1 weighted: 0.85075\n",
      "Test F1 macro: 0.84537\n",
      "\n",
      "Test: Epoch 250\n",
      "Test F1 weighted: 0.85215\n",
      "Test F1 macro: 0.84614\n",
      "\n",
      "Test: Epoch 300\n",
      "Test F1 weighted: 0.86878\n",
      "Test F1 macro: 0.86071\n",
      "\n",
      "Test: Epoch 350\n",
      "Test F1 weighted: 0.86878\n",
      "Test F1 macro: 0.86071\n",
      "Early stop at epoch 373th after 200 epochs not increasing score from epoch 173th with best score 0.8713793922127256\n",
      "****************************************************************************************************\n",
      "idx data: 3\n",
      "200\n",
      "\n",
      "            Config:\n",
      "                * Reproduce:\n",
      "                - Random Seed\n",
      "                    = 42\n",
      "\n",
      "                * Data\n",
      "                - Data Folder\n",
      "                    = /kaggle/input/tcga-gbm-methxgexcnv-2000-3-omics/TCGA_GBM_GExCNVxMETH_2000_MinMaxScaler/3\n",
      "                - Data Train\n",
      "                    = _tr\n",
      "                - Data Test\n",
      "                    = _val\n",
      "                - Saved Model Loc\n",
      "                    = /kaggle/working/models/3\n",
      "                - List Views\n",
      "                    = [1, 2, 3]\n",
      "\n",
      "                *\n",
      "                - Num Epoch PostTrain\n",
      "                    = 1500\n",
      "\n",
      "                - Lr Classifier\n",
      "                    = 0.0001\n",
      "                - Hidden dim\n",
      "                    = [1000]\n",
      "\n",
      "                *\n",
      "                - Test Only\n",
      "                    = False\n",
      "\n",
      "                - Patience\n",
      "                    = 200\n",
      "                - Verbose   \n",
      "                    = False\n",
      "            \n",
      "\n",
      "Training...\n",
      "\n",
      "Test: Epoch 0\n",
      "Test F1 weighted: 0.19874\n",
      "Test F1 macro: 0.19935\n",
      "\n",
      "Test: Epoch 50\n",
      "Test F1 weighted: 0.57933\n",
      "Test F1 macro: 0.52365\n",
      "\n",
      "Test: Epoch 100\n",
      "Test F1 weighted: 0.68589\n",
      "Test F1 macro: 0.63938\n",
      "\n",
      "Test: Epoch 150\n",
      "Test F1 weighted: 0.79481\n",
      "Test F1 macro: 0.78310\n",
      "\n",
      "Test: Epoch 200\n",
      "Test F1 weighted: 0.83663\n",
      "Test F1 macro: 0.83742\n",
      "\n",
      "Test: Epoch 250\n",
      "Test F1 weighted: 0.80034\n",
      "Test F1 macro: 0.80173\n",
      "\n",
      "Test: Epoch 300\n",
      "Test F1 weighted: 0.78277\n",
      "Test F1 macro: 0.78296\n",
      "\n",
      "Test: Epoch 350\n",
      "Test F1 weighted: 0.80230\n",
      "Test F1 macro: 0.79996\n",
      "Early stop at epoch 370th after 200 epochs not increasing score from epoch 170th with best score 0.8366281213340038\n",
      "****************************************************************************************************\n",
      "idx data: 4\n",
      "200\n",
      "\n",
      "            Config:\n",
      "                * Reproduce:\n",
      "                - Random Seed\n",
      "                    = 42\n",
      "\n",
      "                * Data\n",
      "                - Data Folder\n",
      "                    = /kaggle/input/tcga-gbm-methxgexcnv-2000-3-omics/TCGA_GBM_GExCNVxMETH_2000_MinMaxScaler/4\n",
      "                - Data Train\n",
      "                    = _tr\n",
      "                - Data Test\n",
      "                    = _val\n",
      "                - Saved Model Loc\n",
      "                    = /kaggle/working/models/4\n",
      "                - List Views\n",
      "                    = [1, 2, 3]\n",
      "\n",
      "                *\n",
      "                - Num Epoch PostTrain\n",
      "                    = 1500\n",
      "\n",
      "                - Lr Classifier\n",
      "                    = 0.0001\n",
      "                - Hidden dim\n",
      "                    = [1000]\n",
      "\n",
      "                *\n",
      "                - Test Only\n",
      "                    = False\n",
      "\n",
      "                - Patience\n",
      "                    = 200\n",
      "                - Verbose   \n",
      "                    = False\n",
      "            \n",
      "\n",
      "Training...\n",
      "\n",
      "Test: Epoch 0\n",
      "Test F1 weighted: 0.12803\n",
      "Test F1 macro: 0.14020\n",
      "\n",
      "Test: Epoch 50\n",
      "Test F1 weighted: 0.62614\n",
      "Test F1 macro: 0.56707\n",
      "\n",
      "Test: Epoch 100\n",
      "Test F1 weighted: 0.72909\n",
      "Test F1 macro: 0.69438\n",
      "\n",
      "Test: Epoch 150\n",
      "Test F1 weighted: 0.76835\n",
      "Test F1 macro: 0.74950\n",
      "\n",
      "Test: Epoch 200\n",
      "Test F1 weighted: 0.77300\n",
      "Test F1 macro: 0.76384\n",
      "\n",
      "Test: Epoch 250\n",
      "Test F1 weighted: 0.79188\n",
      "Test F1 macro: 0.78021\n",
      "\n",
      "Test: Epoch 300\n",
      "Test F1 weighted: 0.81398\n",
      "Test F1 macro: 0.80843\n",
      "\n",
      "Test: Epoch 350\n",
      "Test F1 weighted: 0.79188\n",
      "Test F1 macro: 0.78021\n",
      "\n",
      "Test: Epoch 400\n",
      "Test F1 weighted: 0.79675\n",
      "Test F1 macro: 0.79230\n",
      "\n",
      "Test: Epoch 450\n",
      "Test F1 weighted: 0.81433\n",
      "Test F1 macro: 0.81372\n",
      "\n",
      "Test: Epoch 500\n",
      "Test F1 weighted: 0.81433\n",
      "Test F1 macro: 0.81372\n",
      "\n",
      "Test: Epoch 550\n",
      "Test F1 weighted: 0.81433\n",
      "Test F1 macro: 0.81372\n",
      "Early stop at epoch 573th after 200 epochs not increasing score from epoch 373th with best score 0.8321468654801988\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "if retrain:\n",
    "    model_folder = '/kaggle/working/models'\n",
    "    for fold_id in idx_list:\n",
    "        print(f'idx data: {fold_id}')\n",
    "\n",
    "        data_folder_idx = data_folder + f'/{fold_id}'\n",
    "        model_folder_idx = model_folder + f'/{fold_id}'\n",
    "\n",
    "        if not bool_using_early_stopping:\n",
    "            patience=None\n",
    "        \n",
    "        print(patience)\n",
    "        !python '{train_file}' '{rseed}' '{data_folder_idx}' \\\n",
    "            '{postfix_tr}' '{postfix_te}' '{model_folder_idx}' \\\n",
    "            '{view_list}' '{num_epoch}'  '{lr}'  '{testonly}' \\\n",
    "            '{hidden_dim}' '{print_hyper}' \\\n",
    "            '{patience}' '{verbose}' \n",
    "        print('*'*100)\n",
    "else:\n",
    "    model_folder = f'/kaggle/input/{dataset_name}/models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2. Load trained models and check accuracy phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T13:26:15.447939Z",
     "iopub.status.busy": "2025-02-09T13:26:15.447686Z",
     "iopub.status.idle": "2025-02-09T13:26:16.759794Z",
     "shell.execute_reply": "2025-02-09T13:26:16.757721Z",
     "shell.execute_reply.started": "2025-02-09T13:26:15.447910Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ModuleAttributeError",
     "evalue": "'MMDynamic' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-9859415efdc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_type_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_type_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_type_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'te'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-860a7025727a>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(bool_report, _type_data)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mcur_model_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{model_folder}/{idx}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mcur_data_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{data_folder}/{idx}/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_data_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_model_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0m_data_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-c5dfe1ea0717>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(data_folder, model_folder)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_model_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m# ---- Done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kaggle/working/nckh_2025/xai/utils.py\u001b[0m in \u001b[0;36mload_model_dict\u001b[0;34m(folder, model_dict)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;31m#            print(\"Module {:} loaded!\".format(module))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mmodel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda:{:}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WARNING: Module {:} from model_dict is not loaded!\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# copy state_dict so _load_from_state_dict can modify it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_metadata'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m         \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m             \u001b[0mstate_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 779\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'MMDynamic' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "evaluate_model(_type_data='tr')\n",
    "evaluate_model(_type_data='val')\n",
    "evaluate_model(_type_data='te')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODULE 2: Integrated Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T13:26:16.760682Z",
     "iopub.status.idle": "2025-02-09T13:26:16.761064Z",
     "shell.execute_reply": "2025-02-09T13:26:16.760886Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "biomarkers_folder = '/kaggle/working/biomarkers/' + COHORT\n",
    "postfix_tr = '_tr'\n",
    "postfix_te = '_val'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1. Baselines for IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T13:26:16.762217Z",
     "iopub.status.idle": "2025-02-09T13:26:16.762657Z",
     "shell.execute_reply": "2025-02-09T13:26:16.762445Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load dict mapping name of {int_id_label:label_name} that int_id_label start from 0 and {int_id_omic:omic_name} that int_id_omic start from 1\n",
    "loc_file_json_id_label = data_folder + '/1/dct_index_subtype.json'\n",
    "loc_file_json_id_omic = data_folder + '/1/dict_id_omics.json'\n",
    "\n",
    "with open(loc_file_json_id_label) as file_json_id_label:\n",
    "    dct_LABEL_MAPPING_NAME = json.load(file_json_id_label)\n",
    "    dct_LABEL_MAPPING_NAME = {int(k): v for k,v in dct_LABEL_MAPPING_NAME.items()} # convert str number key to int\n",
    "# print(\"\\n{int_id_label:label_name} = \", dct_LABEL_MAPPING_NAME)\n",
    "\n",
    "with open(loc_file_json_id_omic) as file_json_id_omic:\n",
    "    dct_OMIC_MAPPING_NAME = json.load(file_json_id_omic)\n",
    "    dct_OMIC_MAPPING_NAME = {int(k): v for k,v in dct_OMIC_MAPPING_NAME.items()} # convert str number key to int\n",
    "# print(\"\\n{int_id_omic:omic_name} = \", dct_OMIC_MAPPING_NAME)\n",
    "\n",
    "\n",
    "\n",
    "###############\n",
    "\n",
    "# may be fixed\n",
    "n_folds = num_models # fold id start from 1\n",
    "type_data = 'tr' # fixed\n",
    "fold_start_id = 1\n",
    "list_fold_id = list(range(n_folds+fold_start_id)[fold_start_id:])\n",
    "\n",
    "\n",
    "dict_fold_dict_baseline = {}\n",
    "\n",
    "for fold_id in list_fold_id:\n",
    "    data_folder_idx = data_folder + f'/{fold_id}'\n",
    "#     print(f'\\n*Using data of fold_id= \"{fold_id}\" with data type = \"{type_data}\" for each omic (total \"{len(dct_OMIC_MAPPING_NAME)}\" type(s) of omic) in folder \"{data_folder_idx}\" to create baseline')\n",
    "    ##############\n",
    "    tmp_label = pd.read_csv(os.path.join(data_folder_idx, f'labels_{type_data}.csv'), header=None)\n",
    "\n",
    "    tmp_dict_omic_pd_data = {id_omic: pd.read_csv(os.path.join(data_folder_idx, f'{str(id_omic)}_{type_data}.csv'), header=None) for id_omic in dct_OMIC_MAPPING_NAME.keys()}\n",
    "    tmp_dict_omic_shape = {id_omic: tmp_dict_omic_pd_data[id_omic].shape[1] for id_omic in dct_OMIC_MAPPING_NAME.keys()} # number of features each omic\n",
    "    \n",
    "    tmp_dict_omic_pd_data_w_label = {id_omic: tmp_dict_omic_pd_data[id_omic].copy(deep=True) for id_omic in dct_OMIC_MAPPING_NAME.keys()}\n",
    "    for id_omic in tmp_dict_omic_pd_data_w_label.keys():\n",
    "        tmp_dict_omic_pd_data_w_label[id_omic]['subtype'] = tmp_label[0]\n",
    "\n",
    "    tmp_dict_baseline= {}\n",
    "    \n",
    "    #############################\n",
    "    tmp_dict_baseline['zeros']= tuple(torch.zeros((1,tmp_dict_omic_shape[id_omic])\n",
    "                                              , dtype=torch.float32) for id_omic in dct_OMIC_MAPPING_NAME.keys())\n",
    "    tmp_dict_baseline['micro_means']= tuple(torch.tensor(tmp_dict_omic_pd_data[id_omic].mean(axis=0).values.reshape(1,-1)\n",
    "                                                         , dtype=torch.float32) for id_omic in dct_OMIC_MAPPING_NAME.keys())\n",
    "    tmp_dict_baseline['macro_means']= tuple(torch.tensor(tmp_dict_omic_pd_data_w_label[id_omic].groupby('subtype').mean().mean(axis=0).values.reshape(1,-1)\n",
    "                                                         , dtype=torch.float32) for id_omic in dct_OMIC_MAPPING_NAME.keys())\n",
    "    ##########################################################\n",
    "\n",
    "    tmp_dict_baseline['dict_default_micro_means']= {}\n",
    "    \n",
    "    tmp_list_of_list_exclude_cursor_label_id = [sorted(list(set(dct_LABEL_MAPPING_NAME.keys()) - set([label_id]))) for label_id in dct_LABEL_MAPPING_NAME.keys()]\n",
    "    tmp_dict_baseline['dict_default_macro_means']= {}\n",
    "    \n",
    "    for label_id in dct_LABEL_MAPPING_NAME.keys():\n",
    "        tmp_dict_baseline['dict_default_macro_means'][label_id] = tuple(\n",
    "            torch.tensor(\n",
    "                tmp_dict_omic_pd_data_w_label[id_omic].groupby('subtype').mean().loc[tmp_list_of_list_exclude_cursor_label_id[label_id]].mean(axis=0).values.reshape(1,-1)\n",
    "                ,dtype=torch.float32\n",
    "            ) for id_omic in dct_OMIC_MAPPING_NAME.keys()\n",
    "        )\n",
    "        \n",
    "        tmp_dict_baseline['dict_default_micro_means'][label_id] = tuple(\n",
    "            torch.tensor(\n",
    "                tmp_dict_omic_pd_data_w_label[id_omic][\n",
    "                    tmp_dict_omic_pd_data_w_label[id_omic]['subtype'] != label_id # filter to exclude row have label is {label_id}\n",
    "                ].loc[:,tmp_dict_omic_pd_data_w_label[id_omic].columns != 'subtype' # then filter to exclude column of label before cal mean for all columns\n",
    "                     ].mean(axis=0).values.reshape(1,-1)\n",
    "                ,dtype=torch.float32\n",
    "            ) for id_omic in dct_OMIC_MAPPING_NAME.keys()\n",
    "        )\n",
    "    ##########################################################\n",
    "     \n",
    "    # Final assign\n",
    "    dict_fold_dict_baseline[fold_id] = tmp_dict_baseline\n",
    "    \n",
    "    print(f'For fold_id = \"{fold_id}\" Done create all baseline type/with name:\\n\\tin {list(tmp_dict_baseline.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T13:26:16.763580Z",
     "iopub.status.idle": "2025-02-09T13:26:16.763981Z",
     "shell.execute_reply": "2025-02-09T13:26:16.763810Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_attribution_scores_for_folder(input, _label, cur_data_folder, idx_model, type_base_line, _n_steps=100):\n",
    "    \n",
    "    # Initialize Integrated Gradients with the custom model predictor\n",
    "    # ig = IntegratedGradients(lambda *inputs: custom_logit_predictor(*inputs, data_folder=cur_data_folder))\n",
    "    ig = GradientShap(lambda *inputs: custom_logit_predictor(*inputs, data_folder=cur_data_folder))\n",
    "\n",
    "    number_of_samples = len(_label)\n",
    "    \n",
    "    # Calculate attribute scores by batch data to avoid running out of memory\n",
    "    # 200 is the maximum (approximate) number of samples that will not cause run out of memory with this TCGA BRCA data (on Train data)\n",
    "    max_samples_per_batch = 100\n",
    "    # Get list end of index to split data into batches\n",
    "    list_end_index = [max_samples_per_batch*times \n",
    "                      for times in range(1,int(np.ceil(number_of_samples/max_samples_per_batch)))\n",
    "                     ] + [number_of_samples]\n",
    "    \n",
    "    attr = {}\n",
    "    for subtype_idx, subtype in enumerate(LABEL_MAPPING_NAME):\n",
    "        #----------------------------------------\n",
    "        start = 0\n",
    "#             print(f'\\n\\t\\t<> Calculate attribution scores with subtype: \"{subtype}\":')\n",
    "\n",
    "\n",
    "        if type_base_line[:4] == 'dict':\n",
    "#                 print(f'\\t\\t Type baseline:\\n\\t\\t  [{type_base_line}] -> Special baseline for each subtype')\n",
    "            baseline = dict_fold_dict_baseline[idx_model][type_base_line][subtype_idx]\n",
    "        else:\n",
    "#                 print(f'\\t\\t Type baseline:\\n\\t\\t  [{type_base_line}]')\n",
    "            baseline = dict_fold_dict_baseline[idx_model][type_base_line]\n",
    "        if cuda:\n",
    "            baseline = tuple(tensor_i.cuda() for tensor_i in baseline)\n",
    "#             print(f'\\t\\t Model pred score:\\n\\t\\t  {custom_logit_predictor(*baseline, data_folder=cur_data_folder).detach().cpu().numpy()} -> SUM={torch.sum(custom_logit_predictor(*baseline, data_folder=cur_data_folder)).detach().cpu().numpy()}')\n",
    "\n",
    "        for end in list_end_index:\n",
    "#                 print(f'\\t\\t\\t:samples from iloc {start} to {end}:')\n",
    "\n",
    "            input_tensor = tuple(input_omic[start:end].requires_grad_() for input_omic in input)\n",
    "\n",
    "            # attr_temp, delta_temp = ig.attribute(input_tensor,\n",
    "            #                                      baselines=baseline,\n",
    "            #                                      target= subtype_idx, return_convergence_delta=True,\n",
    "            #                                     n_steps=_n_steps)\n",
    "\n",
    "            # GradientSHAP\n",
    "            attr_temp, delta_temp = ig.attribute(input_tensor,\n",
    "                                                 baselines=baseline,\n",
    "                                                 target= subtype_idx, return_convergence_delta=True,)\n",
    "            \n",
    "            # concatenate genes attribute score for multi-omics data\n",
    "            attr_temp = np.concatenate(tuple(attr_temp[idx_atr].detach().cpu().numpy() for idx_atr in range(len(attr_temp))), axis=1)\n",
    "            if start == 0:\n",
    "                attr[subtype] =  attr_temp\n",
    "            else:\n",
    "                attr[subtype] = np.concatenate((attr[subtype],attr_temp),axis=0)\n",
    "            start=end\n",
    "#             print(f'\\t\\t\\t: [Delta temp: min={delta_temp.min()}, max={delta_temp.max()}, mean={delta_temp.mean()}, std={delta_temp.std()}]\\n')\n",
    "\n",
    "    return attr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2. Calculate and Sort IG's score of each feature from: train data + trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T13:26:16.764809Z",
     "iopub.status.idle": "2025-02-09T13:26:16.765177Z",
     "shell.execute_reply": "2025-02-09T13:26:16.764995Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "topn=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T13:26:16.766150Z",
     "iopub.status.idle": "2025-02-09T13:26:16.766714Z",
     "shell.execute_reply": "2025-02-09T13:26:16.766426Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def cal_feat_imp(num_models, type_base_line ='zeros',_n_steps=100, type_data='tr'):\n",
    "    \n",
    "    loc_file_json_id_omic = data_folder + '/1/dct_index_subtype.json'\n",
    "    with open(loc_file_json_id_omic) as file_json_id_omic:\n",
    "        dct_LABEL_MAPPING_NAME = json.load(file_json_id_omic)\n",
    "    LABEL_MAPPING_NAME = dct_LABEL_MAPPING_NAME.values() \n",
    "    \n",
    "    start_time=datetime.now()\n",
    "    \n",
    "    dct_feat_imp_by_models = {}\n",
    "    for idx_model in range(1,num_models+1):\n",
    "        data_folder_idx = data_folder + f'/{idx_model}'\n",
    "        model_folder_idx = model_folder + f'/{idx_model}'\n",
    "        added_softmax = True\n",
    "        \n",
    "        # loading dataset\n",
    "        _data_list=[]\n",
    "        _label = np.loadtxt(os.path.join(data_folder_idx, f\"labels_{type_data}.csv\"), delimiter=',').astype(int)\n",
    "\n",
    "        for i in view_list:\n",
    "            _data_loc = os.path.join(data_folder_idx, str(i)+ f\"_{type_data}.csv\")\n",
    "            _data_list.append(np.loadtxt(_data_loc, delimiter=','))\n",
    "        _tensor_data_list = tuple(torch.tensor(np_arr,dtype=torch.float32).to(device) for np_arr in _data_list)\n",
    "\n",
    "        gene_name = []\n",
    "        for v in view_list:\n",
    "            df = pd.read_csv(os.path.join(data_folder_idx, str(v)+\"_featname.csv\"), header=None)    \n",
    "            gene_name.extend(df[0].values.tolist())\n",
    "            # DONE USE THIS: gene_name.extend(df[0].str.split(r'\\|').str[0].values.tolist()) # only used when want combined same features that first part before | symbol if exist\n",
    "\n",
    "        \n",
    "        ## load models weights\n",
    "        load_model(data_folder_idx, model_folder_idx)\n",
    "        \n",
    "        #---------------------------------------------------------------------\n",
    "\n",
    "        # Calculate attribute score:\n",
    "        attr = calculate_attribution_scores_for_folder(_tensor_data_list, _label, data_folder_idx, idx_model, type_base_line, _n_steps)\n",
    "        \n",
    "        # ------------------------------\n",
    "        lst_ordered_subtype = list(attr.keys())\n",
    "        stack_subtype_attr = np.stack(tuple(attr[subtype] for subtype in lst_ordered_subtype))\n",
    "        feat_imp_like_each_class = np.abs(stack_subtype_attr).mean(axis=1)\n",
    "        feat_imp_like = feat_imp_like_each_class.sum(axis=0)\n",
    "        \n",
    "        dct_feat_imp_by_models[idx_model] = feat_imp_like\n",
    "    \n",
    "    # along models\n",
    "    final_feat_imp_like = np.mean(list(dct_feat_imp_by_models.values()), axis=0)\n",
    "\n",
    "    idx_sorted_desc_mean_abs_sum = np.argsort(final_feat_imp_like)[::-1] # Note hien tai dang khong quan tam examples nao dung, example nao du doan sai nhu mot so cach o duoi day\n",
    "    sorted_score_feat_imp_like = final_feat_imp_like[idx_sorted_desc_mean_abs_sum]\n",
    "    rank_imp_feats_by_abs_mean_sum_without_combine_same_gene_name = list(np.array(gene_name)[idx_sorted_desc_mean_abs_sum])\n",
    "    pd_rank_by_abs_mean_sum = pd.DataFrame({'gene_name': rank_imp_feats_by_abs_mean_sum_without_combine_same_gene_name}) #, 'score':sorted_score_feat_imp_like\n",
    "    print(f'\\t=>Result (Here print out the top {topn} biomakers with highest values):')\n",
    "    \n",
    "    end_time=datetime.now()\n",
    "    print(f'\\tRun in {end_time-start_time}')\n",
    "    \n",
    "    display(pd_rank_by_abs_mean_sum.head(topn))\n",
    "    return pd_rank_by_abs_mean_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T13:26:16.767849Z",
     "iopub.status.idle": "2025-02-09T13:26:16.768371Z",
     "shell.execute_reply": "2025-02-09T13:26:16.768115Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def export_biomarker_file(list_concerned_type_baseline, list_concerned_type_data, list_concerned_num_models, biomarkers_folder, _n_steps=100):\n",
    "    list_biomakers_file_loc = []\n",
    "    print('_'*100)\n",
    "    print(f'Biomarker Discovery for {biomarkers_folder.split(\"/\")[-1]} cohort:')\n",
    "    pd_results = {}\n",
    "    for type_base_line in list_concerned_type_baseline:\n",
    "        pd_results[type_base_line] = {}\n",
    "        for type_data in list_concerned_type_data:\n",
    "            pd_results[type_base_line][type_data] = {}\n",
    "            for num_models in list_concerned_num_models:\n",
    "                \n",
    "                pd_results[type_base_line][type_data][num_models] = cal_feat_imp(num_models=num_models,type_base_line=type_base_line,_n_steps=n_steps, type_data= type_data)\n",
    "\n",
    "                if not os.path.exists(biomarkers_folder):\n",
    "                    os.makedirs(biomarkers_folder)\n",
    "                    \n",
    "                biomakers_file_loc = biomarkers_folder + f'/IG_{type_base_line}_{type_data}_{num_models}_{data_folder.split(\"/\")[-1]}.csv'\n",
    "                pd_results[type_base_line][type_data][num_models][['gene_name']].to_csv(biomakers_file_loc, index=False)\n",
    "                list_biomakers_file_loc.append(biomakers_file_loc)\n",
    "\n",
    "    print()\n",
    "    return list_biomakers_file_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T13:26:16.769539Z",
     "iopub.status.idle": "2025-02-09T13:26:16.770093Z",
     "shell.execute_reply": "2025-02-09T13:26:16.769831Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# list_concerned_type_baseline = ['zeros', 'micro_means', 'macro_means', 'dict_default_micro_means', 'dict_default_macro_means']\n",
    "list_concerned_type_baseline = ['zeros']\n",
    "list_concerned_type_data = ['tr']\n",
    "list_concerned_num_models = [num_models]\n",
    "\n",
    "n_steps=100\n",
    "\n",
    "list_biomakers_file_loc = export_biomarker_file(list_concerned_type_baseline, list_concerned_type_data, list_concerned_num_models, biomarkers_folder, n_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3. MOGONET(NO IG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T13:26:16.771266Z",
     "iopub.status.idle": "2025-02-09T13:26:16.771910Z",
     "shell.execute_reply": "2025-02-09T13:26:16.771533Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if Run_MMD:\n",
    "    biomarker_file_name = f'mogonet_full_top_biomarkers_sorted_desc_score_{num_models}models.csv'\n",
    "    main_biomarker_mogonet = '/kaggle/working/nckh_2025/xai/main_biomarker.py'\n",
    "\n",
    "    start_time=datetime.now()\n",
    "    print(main_biomarker_mogonet)\n",
    "    !python '{main_biomarker_mogonet}' '{data_folder}' '{model_folder}' '{dim_he_list}' '{view_list}' '{num_models}' '{postfix_tr}' '{postfix_te}' '{biomarkers_folder}' '{biomarker_file_name}' '{topn}'\n",
    "    print(f'\\t=>Result (Here print out the top {topn} biomakers with highest values):')\n",
    "    end_time=datetime.now()\n",
    "    print(f'\\tRun in {end_time-start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. PHASE 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T13:26:16.773173Z",
     "iopub.status.idle": "2025-02-09T13:26:16.773746Z",
     "shell.execute_reply": "2025-02-09T13:26:16.773445Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Threshold to take number of genes (biomarkers) per subtype\n",
    "print(dct_OMIC_MAPPING_NAME)\n",
    "ROOT_DATA_FOLDER = f'/kaggle/input/{dataset_name}/{COHORT}/train_test_split_org/'\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "LIST_OMICS = list(dct_OMIC_MAPPING_NAME.values())\n",
    "print(LIST_OMICS)\n",
    "LIST_OMICS_ID = np.arange(1,len(LIST_OMICS)+1,1)\n",
    "\n",
    "# Single omic or Multi-omics| to run experiments\n",
    "LIST_EXP_OMICS = ['_'.join(LIST_OMICS)]\n",
    "print(LIST_EXP_OMICS)\n",
    "\n",
    "\n",
    "LIST_TYPE_DATA = ['train', 'test']\n",
    "DATA_FOLDER = {'train': ROOT_DATA_FOLDER,\n",
    "              'test': ROOT_DATA_FOLDER}\n",
    "\n",
    "\n",
    "loc_file_json_id_label = data_folder + '/1/dct_index_subtype.json'\n",
    "with open(loc_file_json_id_label) as file_json_id_label:\n",
    "    dct_LABEL_MAPPING_NAME = json.load(file_json_id_label)\n",
    "    dct_LABEL_MAPPING_NAME = {int(k): v for k,v in dct_LABEL_MAPPING_NAME.items()}\n",
    "ORIGINAL_MAPPING_NAME = dct_LABEL_MAPPING_NAME\n",
    "print(ORIGINAL_MAPPING_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T13:26:16.774901Z",
     "iopub.status.idle": "2025-02-09T13:26:16.775442Z",
     "shell.execute_reply": "2025-02-09T13:26:16.775175Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BIOMARKERS_RESULT_FOLDER = '/kaggle/working/biomarkers'\n",
    "list_loc_biomarkers = []\n",
    "for dirname, _, filenames in os.walk(BIOMARKERS_RESULT_FOLDER):\n",
    "    for filename in filenames:\n",
    "        list_loc_biomarkers.append(os.path.join(dirname, filename))\n",
    "# print(list_loc_biomarkers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1. Check overlap with other genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T13:26:16.776593Z",
     "iopub.status.idle": "2025-02-09T13:26:16.777143Z",
     "shell.execute_reply": "2025-02-09T13:26:16.776887Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "direct_evidence_gene_list = [\n",
    "    \"EGFR\", \"MGMT\", \"CXCL8\", \"H2AX\", \"VEGFA\", \"EGF\", \"MMP2\", \"HIF1A\", \"MMP9\", \"IL1B\",\n",
    "    \"MYC\", \"CDK6\", \"MTOR\", \"HES1\", \"FN1\", \"NDRG1\", \"CTSB\", \"PTK2\", \"CD9\", \"PROM1\",\n",
    "    \"WT1\", \"FAT1\", \"HEY1\", \"DUSP6\", \"NCOR1\", \"NOTCH1\", \"BMI1\", \"BHLHE40\", \"TP53BP1\", \"PIM1\",\n",
    "    \"MET\", \"JAG1\", \"CSTA\", \"CHI3L1\", \"APC\", \"CTSK\", \"NOTCH2\", \"FAM83D\", \"GSTT1\", \"FOSL2\",\n",
    "    \"GDNF\", \"TES\", \"PRKN\", \"SUZ12\", \"CSTB\", \"ZBTB7A\", \"BCHE\", \"RUNX1\", \"CCNH\", \"LRRC59\",\n",
    "    \"PML\", \"POLK\", \"SRRT\", \"RUNX3\", \"CTNND2\", \"TMEM135\", \"MDM4\", \"H3-3A\", \"BRD2\", \"TRMT11\",\n",
    "    \"NF1\", \"HOXD10\", \"GRIK2\", \"JAG2\", \"BRD4\", \"LTBP4\", \"MACIR\", \"NOTCH3\", \"LZTR1\", \"H3C2\",\n",
    "    \"SLC22A10\", \"DEUP1\", \"SEPTIN14\", \"TGM2\", \"TNFSF10\", \"IL2\", \"RECK\", \"IFNA2\"\n",
    "]\n",
    "topn_lst = [50, 100, 200, 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T13:26:16.778303Z",
     "iopub.status.idle": "2025-02-09T13:26:16.778735Z",
     "shell.execute_reply": "2025-02-09T13:26:16.778512Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for file_loc in list_loc_biomarkers:\n",
    "    file_name = file_loc.split('/')[-1]\n",
    "    ig_biomarkers = pd.read_csv(file_loc).iloc[:, 0].str.split(r'\\|').str[0].values.tolist()\n",
    "    baseline_name = '-'.join(file_name.split('_')[:-7])\n",
    "    for i in topn_lst:\n",
    "        intersect_direct = sorted(list(set(direct_evidence_gene_list).intersection(set(ig_biomarkers[:i]))))\n",
    "        intersect_direct_str = ', '.join(intersect_direct)\n",
    "        data.append([baseline_name, i, intersect_direct_str]) #, intersect_reference, intersect_inference])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Baseline', 'TopN', 'Intersect with Direct Evidence Gene List']) #, 'Intersect with Top Reference Gene List', 'Intersect with Top Inference Gene List'])\n",
    "\n",
    "# Print table\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(df)\n",
    "\n",
    "df.to_csv(\"/kaggle/working/genes_overlap.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2. TEST WITH CLASSIC ML ALGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T13:26:16.779482Z",
     "iopub.status.idle": "2025-02-09T13:26:16.779886Z",
     "shell.execute_reply": "2025-02-09T13:26:16.779712Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.expand_frame_repr', False)\n",
    "# pd.set_option('max_colwidth', None)\n",
    "def printmd(string, color=None):\n",
    "    colorstr = \"<span style='color:{}'>{}</span>\".format(color, string)\n",
    "    display(Markdown(colorstr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T13:26:16.780670Z",
     "iopub.status.idle": "2025-02-09T13:26:16.781006Z",
     "shell.execute_reply": "2025-02-09T13:26:16.780850Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def display_classification_report(\n",
    "    n_class,\n",
    "    label,\n",
    "    pred,\n",
    "    label_mapping_name,\n",
    "    cmap=\"Blues\",\n",
    "    fmt=\".2%\",\n",
    "    annot=True,\n",
    "    path=None,  # str path to save fig. If not None\n",
    "    shown=True,\n",
    "):\n",
    "\n",
    "    clf_report = classification_report(\n",
    "        label,\n",
    "        pred,\n",
    "        target_names=label_mapping_name,\n",
    "        digits=4,\n",
    "        zero_division=0,\n",
    "        output_dict=True,\n",
    "    )\n",
    "\n",
    "    clf_df = pd.DataFrame(clf_report)\n",
    "    clf_df.loc[[\"precision\", \"recall\"], \"accuracy\"] = np.nan\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_figwidth(12)\n",
    "    ConfusionMatrixDisplay(\n",
    "        confusion_matrix(label, pred), display_labels=label_mapping_name\n",
    "    ).plot(cmap=cmap, ax=ax1)\n",
    "    sns.heatmap(\n",
    "        clf_df.iloc[:-1, :].T, annot=annot, cmap=cmap, robust=True, ax=ax2, fmt=fmt\n",
    "    )\n",
    "    if path is not None:\n",
    "        fig.savefig(path, dpi=300)\n",
    "    if shown:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T13:26:16.782083Z",
     "iopub.status.idle": "2025-02-09T13:26:16.782472Z",
     "shell.execute_reply": "2025-02-09T13:26:16.782292Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tuning_and_eval(gridcvs, X_train, y_train, X_test, y_test,\n",
    "                    scoring, refit, is_binary_problem,\n",
    "                    result_on_dataset, rank_hparams_info):\n",
    "    assert 'test' in result_on_dataset\n",
    "    assert isinstance(rank_hparams_info, bool)\n",
    "    ###\n",
    "    lst_dct_result = [] # to return\n",
    "    ###\n",
    "\n",
    "    start=datetime.now()\n",
    "    X = {}\n",
    "    y = {}\n",
    "    X['train'] = X_train\n",
    "    X['test'] = X_test\n",
    "    y['train'] = y_train\n",
    "    y['test'] = y_test\n",
    "\n",
    "    for model_name, gs_est in sorted(gridcvs.items()):\n",
    "        ###\n",
    "        sub_result = {}\n",
    "        sub_result['model'] = model_name\n",
    "        ###\n",
    "\n",
    "        start_individual_type_model = datetime.now()\n",
    "        gs_est.fit(X['train'],y['train'])\n",
    "\n",
    "        ###\n",
    "        sub_result['best_params'] = gs_est.best_params_\n",
    "        ###\n",
    "\n",
    "        ###\n",
    "        sub_result[f'best_tuning_{refit}'] = gs_est.best_score_ * 100\n",
    "        sub_result['best_tuning_std'] = gs_est.cv_results_[f'std_test_{refit}'][gs_est.best_index_] * 100\n",
    "        ###\n",
    "\n",
    "        if rank_hparams_info:\n",
    "            select_result_cols = []\n",
    "            for metric in scoring:\n",
    "                select_result_cols.extend(['rank_test_'+metric,'mean_test_'+ metric, 'std_test_'+metric])\n",
    "            select_result_cols.extend(['params'])\n",
    "\n",
    "            dataframe_results = pd.DataFrame(gs_est.cv_results_).loc[:,select_result_cols].sort_values(by=f'mean_test_{refit}',ascending=False)\n",
    "            display(dataframe_results[:10])\n",
    "\n",
    "        for type_data in result_on_dataset:\n",
    "            y_predict = gs_est.predict(X[type_data])\n",
    "\n",
    "            acc = accuracy_score(y_true=y[type_data], y_pred=y_predict)\n",
    "\n",
    "            ###\n",
    "            sub_result[f'{type_data}_acc'] = acc * 100\n",
    "            ###\n",
    "\n",
    "            if is_binary_problem:\n",
    "                f1 = f1_score(y_true=y[type_data], y_pred=y_predict,average='binary')\n",
    "                y_score = gs_est.predict_proba(X[type_data])[:, 1]\n",
    "                roc_auc = roc_auc_score(y_true=y[type_data], y_score=y_score)\n",
    "\n",
    "                ###\n",
    "                sub_result[f'{type_data}_f1'] = f1 * 100\n",
    "                ###\n",
    "\n",
    "                ###\n",
    "                sub_result[f'{type_data}_roc_auc'] = roc_auc * 100\n",
    "                ###\n",
    "            else:\n",
    "                f1_macro = f1_score(y_true=y[type_data], y_pred=y_predict,average='macro')\n",
    "                f1_weighted = f1_score(y_true=y[type_data], y_pred=y_predict,average='weighted')\n",
    "\n",
    "                ###\n",
    "                sub_result[f'{type_data}_f1_macro'] = f1_macro * 100\n",
    "                ###\n",
    "\n",
    "                ###\n",
    "                sub_result[f'{type_data}_f1_weighted'] = f1_weighted * 100\n",
    "                ##\n",
    "\n",
    "            ###\n",
    "            lst_dct_result.append(sub_result)\n",
    "            ###\n",
    "\n",
    "#             pd_cfm = pd.crosstab(\n",
    "#                 y[type_data]\n",
    "#                 , y_predict\n",
    "#                 , margins=True\n",
    "#                 , rownames=['True label']\n",
    "#                 , colnames=['Pred label']\n",
    "#             )\n",
    "            \n",
    "#             for label in ORIGINAL_MAPPING_NAME.values():\n",
    "#                 if label not in pd_cfm.index:\n",
    "#                     pd_cfm.loc[label] = 0\n",
    "#                 if label not in pd_cfm.columns:\n",
    "#                     pd_cfm[label] = 0\n",
    "\n",
    "#             pd_cfm = pd_cfm.reindex(index=list(ORIGINAL_MAPPING_NAME.values()) + ['All'], \n",
    "#                                     columns=list(ORIGINAL_MAPPING_NAME.values()) + ['All'])\n",
    "\n",
    "#             display(pd_cfm)\n",
    "            \n",
    "            folder_save_fig = f\"/kaggle/working/cfm/{biomarker_file.split('/')[-1].split('.')[-2]}/{model_name}\"\n",
    "            if not os.path.exists(folder_save_fig):\n",
    "                os.makedirs(folder_save_fig)\n",
    "            path_save_fig = f\"{folder_save_fig}/top{threshold}.png\"\n",
    "            display_classification_report(n_class=len(ORIGINAL_MAPPING_NAME)\n",
    "                                          , label=y[type_data]\n",
    "                                          , pred= y_predict\n",
    "                                          , label_mapping_name=ORIGINAL_MAPPING_NAME.values()\n",
    "                                          , path=path_save_fig\n",
    "                                          , shown=False\n",
    "                                         )\n",
    "    return lst_dct_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T13:26:16.783471Z",
     "iopub.status.idle": "2025-02-09T13:26:16.783943Z",
     "shell.execute_reply": "2025-02-09T13:26:16.783734Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validate_biomarker(dict_X_train, dict_y_train, dict_X_test, dict_y_test,\n",
    "                       omics=['GE_CNA', 'GE','CNA'], random_state = RANDOM_STATE,\n",
    "                       result_on_dataset = ['train','test'], rank_hparams_info = True,\n",
    "                       is_binary_problem=False):\n",
    "    assert 'test' in result_on_dataset\n",
    "    assert isinstance(rank_hparams_info, bool)\n",
    "    ###\n",
    "    validate_biomarker_result = []\n",
    "    ###\n",
    "    scoring = None\n",
    "    refit= None\n",
    "    if is_binary_problem:\n",
    "        scoring = ['f1','accuracy','roc_auc']\n",
    "        refit = 'f1'\n",
    "    else: \n",
    "        scoring = ['f1_macro','f1_weighted', 'accuracy']\n",
    "        refit = 'f1_macro'\n",
    "\n",
    "    # Initializing classifiers\n",
    "    clf1 = LogisticRegression(random_state=random_state, max_iter=10000, n_jobs=-1)\n",
    "\n",
    "    # Binary case, probability = True to cal ROC_AUC, slowdown k-fold....\n",
    "    clf2 = SVC(random_state=random_state, probability=is_binary_problem)\n",
    "\n",
    "    clf3 = RandomForestClassifier(random_state=random_state,n_jobs=-1)\n",
    "\n",
    "    # Building the pipelines\n",
    "    pipe1 = Pipeline([('std', 'passthrough'),\n",
    "                      ('clf1', clf1)])\n",
    "\n",
    "    pipe2 = Pipeline([('std', 'passthrough'),\n",
    "                      ('clf2', clf2)])\n",
    "\n",
    "#     # only apply std to mRNA data/ BY index mRNA| ignore or passthorough not to\n",
    "#     # apply standard scaler to remaining index corresponding to CNA data\n",
    "#     column_trans = ColumnTransformer(\n",
    "#         [('scaler', StandardScaler(),list(range(len(GENE['mRNA']))))]\n",
    "#         ,remainder='passthrough')\n",
    "    # Setting up the parameter grids\n",
    "    param_grid1 = [{\n",
    "                    'std': [MinMaxScaler()],\n",
    "                    'clf1__penalty': ['l2'],\n",
    "                    'clf1__multi_class':[\"multinomial\"],\n",
    "                    'clf1__solver':[\"newton-cg\"],\n",
    "                    'clf1__class_weight': [\"balanced\"],\n",
    "                    'clf1__C': np.power(10., np.arange(-4, 3)),\n",
    "                    }]\n",
    "\n",
    "    param_grid2 = [{\n",
    "                    'std': [MinMaxScaler()],\n",
    "                    'clf2__kernel': ['rbf'],\n",
    "                    'clf2__class_weight': [\"balanced\"],\n",
    "                    'clf2__C': np.power(10., np.arange(-4, 3)),\n",
    "                    'clf2__gamma': list(np.power(10., np.arange(-4, 0))) + ['scale']\n",
    "                    }]\n",
    "\n",
    "    param_grid3 = [{'n_estimators': [50, 100, 150],\n",
    "                    'max_features': [\"sqrt\"],\n",
    "                    'max_depth' : list(range(1, 10)) + [None],\n",
    "                    'criterion' :[\"gini\"],\n",
    "                    'class_weight': [\"balanced\", \"balanced_subsample\"]}]\n",
    "\n",
    "    # Setting up multiple GridSearchCV objects, 1 for each algorithm\n",
    "    gridcvs = {}\n",
    "#     cv = RepeatedStratifiedKFold(n_splits=4, n_repeats=10, random_state=random_state)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=4, n_repeats=5, random_state=random_state)\n",
    "\n",
    "    train_options = zip(\n",
    "                        (param_grid1,\n",
    "                         param_grid2,\n",
    "                         param_grid3,\n",
    "                        ),\n",
    "                        (pipe1,\n",
    "                         pipe2,\n",
    "                         clf3,\n",
    "                        ),\n",
    "                        ('1_Softmax',\n",
    "                         '2_SVM',\n",
    "                         '3_RandomForest',\n",
    "                        )\n",
    "                       )\n",
    "\n",
    "    for pgrid, est, model_name in train_options:\n",
    "        gcv = GridSearchCV(estimator=est,\n",
    "                           param_grid=pgrid,\n",
    "                           scoring=scoring,\n",
    "                           n_jobs=-1,\n",
    "                           cv=cv,\n",
    "                           verbose=0,\n",
    "                           refit=refit)\n",
    "        gridcvs[model_name] = gcv\n",
    "\n",
    "    for omic in omics:\n",
    "#         print('-'*100)\n",
    "#         printmd(f'Validate on {omic} data:\\n', color=\"red\")\n",
    "\n",
    "        X_train = dict_X_train[omic]\n",
    "        y_train = np.array(dict_y_train[omic], dtype=np.int16)\n",
    "#         print('Train dist: ', np.unique(y_train, return_counts=True ))\n",
    "\n",
    "        X_test = dict_X_test[omic]\n",
    "        y_test = np.array(dict_y_test[omic], dtype=np.int16)\n",
    "#         print('Test dist', np.unique(y_test, return_counts=True ),'\\n')\n",
    "\n",
    "        # run tuning and eval\n",
    "        tmp_lst_dct_tuning_result = tuning_and_eval(gridcvs, X_train, y_train, X_test, y_test,\\\n",
    "                        scoring, refit,is_binary_problem,\n",
    "                        result_on_dataset, rank_hparams_info)\n",
    "        ###\n",
    "        tmp_base= {'using_omic': omic}\n",
    "        validate_biomarker_result.extend([copy.deepcopy(tmp_base) for i in range(len(tmp_lst_dct_tuning_result))])\n",
    "        for dct_tmp, dct_val in zip(tmp_lst_dct_tuning_result, validate_biomarker_result):\n",
    "            dct_val.update(dct_tmp)\n",
    "        ###\n",
    "    return validate_biomarker_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T13:26:16.784810Z",
     "iopub.status.idle": "2025-02-09T13:26:16.785141Z",
     "shell.execute_reply": "2025-02-09T13:26:16.784989Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validate(biomarker_file, threshold):\n",
    "    # init var and para to save the result\n",
    "    ###\n",
    "    validate_result=[]\n",
    "    base_result = {}\n",
    "    ###\n",
    "#     dct_data_structure = {col: [] for col in lst_cols}\n",
    "#     pd_result = pd.DataFrame(columns=lst_cols)\n",
    "\n",
    "    # Read data\n",
    "    dict_df_label = {}\n",
    "    dict_df_data = {}\n",
    "    # Read data as df and create numpy array data for labeled data\n",
    "    for type_data in LIST_TYPE_DATA:\n",
    "        # modified\n",
    "        dict_df_label[type_data] = pd.read_csv(DATA_FOLDER[type_data] + f'labels_{type_data[:2]}.csv', names=['disease_subtypes'])\n",
    "        # ---------------------------------\n",
    "\n",
    "        # added\n",
    "        dict_df_label[type_data]['disease_subtypes'] = dict_df_label[type_data]['disease_subtypes'].astype('int')\n",
    "        dict_df_label[type_data].index.names = ['sampleID']\n",
    "        \n",
    "        dict_df_label[type_data].replace({'disease_subtypes': ORIGINAL_MAPPING_NAME}, inplace=True)\n",
    "        # ---------------------------------\n",
    "        \n",
    "        \n",
    "        dict_df_omics = {}\n",
    "        dict_narray_omics = {}\n",
    "        for omic in LIST_OMICS:\n",
    "            # added\n",
    "            tmp_feat_name = pd.read_csv(DATA_FOLDER[type_data]+ f'{LIST_OMICS.index(omic)+1}_featname.csv', names=['feat_name'])\n",
    "#             tmp_feat_name['feat_name'] = tmp_feat_name['feat_name'].str.split('|').str[0]\n",
    "            lst_name = tmp_feat_name.values.reshape(-1).tolist()\n",
    "            # ---------------------------------\n",
    "\n",
    "            # modified\n",
    "            dict_df_omics[omic] = pd.read_csv(DATA_FOLDER[type_data] + f'{LIST_OMICS.index(omic)+1}_{type_data[:2]}.csv',names=lst_name)\n",
    "            # ---------------------------------\n",
    "\n",
    "        dict_df_data[type_data] = dict_df_omics\n",
    "\n",
    "    LABEL_MAPPING_NAME = dict_df_label['train']['disease_subtypes'].astype('category').cat.categories # sorted by alphabetical order\n",
    "#     print('LABEL_MAPPING_NAME', LABEL_MAPPING_NAME)\n",
    "    \n",
    "    # Convert categorical label to numerical label\n",
    "    for type_data in LIST_TYPE_DATA:\n",
    "        dict_df_label[type_data].loc[:,'disease_subtypes'] = dict_df_label[type_data]['disease_subtypes'].astype('category').cat.codes\n",
    "\n",
    "    #---------------------------------------------------------------------------------------\n",
    "    # Keep only biomarker genes found from TCGA data\n",
    "#     print('-'*100)\n",
    "#     print('KEEP ONLY BIOMARKER GENES FOUND FROM TCGA DATA')\n",
    "    score_genes = pd.read_csv(biomarker_file)\n",
    "    score_genes = score_genes.iloc[:threshold, 0]\n",
    "    top_genes = list(set(score_genes.to_numpy(copy=True).reshape(-1)))\n",
    "#     print(top_genes)\n",
    "    top_genes = [gene.upper() for gene in top_genes]\n",
    "#     print(f'Top {threshold} from TCGA have {len(top_genes)} unique genes/features:')\n",
    "    ###\n",
    "    base_result['n_unq_markers'] = len(top_genes)\n",
    "    base_result['lst_unq_markers'] = top_genes\n",
    "    ###\n",
    "    \n",
    "    GENE = {}\n",
    "    for omic in LIST_OMICS:\n",
    "        GENE[omic] = dict_df_data['train'][omic].columns[\n",
    "#             dict_df_data['train'][omic].columns.str.upper().str.split(r'\\|').str[0].isin(top_genes)\n",
    "            dict_df_data['train'][omic].columns.str.upper().isin(top_genes)\n",
    "        ].to_numpy(copy=True).tolist()\n",
    "\n",
    "#         print(f'\\twith {omic} TOP {threshold}:', len(GENE[omic]))\n",
    "        ###\n",
    "        base_result[f'n_unq_{omic}'] = len(GENE[omic])\n",
    "        base_result[f'lst_unq_{omic}'] = GENE[omic]\n",
    "        ###\n",
    "    # NOTE THAT DNAmythyl and mRNA maybe have same genename in top gene => incresing num features comparing to num unique genes\n",
    "\n",
    "        for type_data in LIST_TYPE_DATA:\n",
    "            dict_df_data[type_data][omic] = dict_df_data[type_data][omic][GENE[omic]].copy(deep=True)\n",
    "    \n",
    "    dict_X = {}\n",
    "    dict_y = {}\n",
    "    for type_data in LIST_TYPE_DATA:\n",
    "        dict_X[type_data] = {}\n",
    "        dict_y[type_data] = {}\n",
    "\n",
    "    for type_omic in LIST_EXP_OMICS:\n",
    "        if '_' in type_omic:\n",
    "#             print(f'Creating data for multi-omics experiment: {type_omic}')\n",
    "            list_omics = type_omic.split('_')\n",
    "            for type_data in LIST_TYPE_DATA:\n",
    "                tuple_data_omics = tuple([dict_df_data[type_data][single_omic] for single_omic in list_omics])\n",
    "                dict_X[type_data][type_omic] = np.concatenate(tuple_data_omics, axis=1)\n",
    "        else:\n",
    "#             print(f'Creating data for single omic experiment: {type_omic}')\n",
    "            for type_data in LIST_TYPE_DATA:\n",
    "                dict_X[type_data][type_omic] = dict_df_data[type_data][type_omic].to_numpy(copy=True)\n",
    "\n",
    "        for type_data in LIST_TYPE_DATA:\n",
    "            dict_y[type_data][type_omic] = dict_df_label[type_data]['disease_subtypes'].to_numpy(copy=True)\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------\n",
    "    tmp_lst_dct_validate_biomarker_result = validate_biomarker(dict_X['train'], dict_y['train'], dict_X['test'], dict_y['test'],\n",
    "                       omics=LIST_EXP_OMICS, random_state=RANDOM_STATE,\n",
    "                       result_on_dataset= ['test'], rank_hparams_info =False,\n",
    "                       is_binary_problem = (len(LABEL_MAPPING_NAME)==2))\n",
    "    validate_result.extend([copy.deepcopy(base_result) for i in range(len(tmp_lst_dct_validate_biomarker_result))])\n",
    "    for dct_tmp, dct_val in zip(tmp_lst_dct_validate_biomarker_result, validate_result):\n",
    "        dct_val.update(dct_tmp)\n",
    "    return validate_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T13:26:16.786255Z",
     "iopub.status.idle": "2025-02-09T13:26:16.786599Z",
     "shell.execute_reply": "2025-02-09T13:26:16.786435Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# excluded_files = ['mogonet_full_top_biomarkers_sorted_desc_score_5models.csv']\n",
    "# excluded_files = [f'{BIOMARKERS_RESULT_FOLDER}/{COHORT}/'+biomarker_file for biomarker_file in excluded_files]\n",
    "\n",
    "excluded_files = [] # evaluation all candidate biomarkers result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T13:26:16.787744Z",
     "iopub.status.idle": "2025-02-09T13:26:16.788071Z",
     "shell.execute_reply": "2025-02-09T13:26:16.787918Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(pd.__version__)\n",
    "print(sklearn.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T13:26:16.788966Z",
     "iopub.status.idle": "2025-02-09T13:26:16.789317Z",
     "shell.execute_reply": "2025-02-09T13:26:16.789152Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Ignore ConvergenceWarning\n",
    "# warnings.filterwarnings(\"ignore\", category = ConvergenceWarning)\n",
    "\n",
    "##\n",
    "result = []\n",
    "THRESHOLD_LST = list(range(25, 401, 25))\n",
    "##\n",
    "\n",
    "for biomarker_file in list_loc_biomarkers:\n",
    "    if biomarker_file in excluded_files:\n",
    "        continue\n",
    "#     print(\"*\"*100)\n",
    "    baseline = biomarker_file.split('/')[-1].split('.')[-2]\n",
    "    printmd(baseline,'green')\n",
    "#         print(biomarker_file)\n",
    "\n",
    "    start = datetime.now()\n",
    "    for threshold in THRESHOLD_LST:\n",
    "        ###\n",
    "        tmp_result = []\n",
    "        base_init_dct_result = {'top': threshold, 'baseline': baseline}\n",
    "        tmp_validate_result = validate(biomarker_file,threshold)\n",
    "        tmp_result.extend([copy.deepcopy(base_init_dct_result) for i in range(len(tmp_validate_result))])\n",
    "        for dct_tmp, dct_val in zip(tmp_validate_result, tmp_result):\n",
    "            dct_val.update(dct_tmp)\n",
    "\n",
    "        result.extend(tmp_result)\n",
    "        \n",
    "    print(f'Total Time: {datetime.now()-start}')\n",
    "        ###\n",
    "#         print(f\"Top {threshold} - Using {tmp_result[0]['n_unq_markers']} uniques biomarkers in totals\")\n",
    "#         print(tmp_result[0]['lst_unq_markers'])\n",
    "#         print('\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3. Compare all IG baselines and MOGONET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T13:26:16.790537Z",
     "iopub.status.idle": "2025-02-09T13:26:16.790978Z",
     "shell.execute_reply": "2025-02-09T13:26:16.790782Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "avg_acc_all_models = {}\n",
    "avg_f1_all_models = {}\n",
    "filtered_results = []\n",
    "\n",
    "for res in result:\n",
    "#         print(res)\n",
    "    model_type = res['model']\n",
    "    baseline_type = \"-\".join(res['baseline'].split('_')[:-7])\n",
    "    top_n = res['top']\n",
    "    accuracy_t = res['test_acc']\n",
    "    f1_score_t = res.get('test_f1_macro', res.get('test_f1', 0))\n",
    "\n",
    "    filtered_results.append({\n",
    "        'model_type': model_type,\n",
    "        'baseline_type': baseline_type,\n",
    "        'top_n': top_n,\n",
    "        'accuracy_t': accuracy_t,\n",
    "        'f1_score_t': f1_score_t\n",
    "    })\n",
    "    \n",
    "    if model_type not in avg_acc_all_models:\n",
    "        avg_acc_all_models[model_type] = {}\n",
    "        avg_f1_all_models[model_type] = {}\n",
    "\n",
    "    if baseline_type not in avg_acc_all_models[model_type]:\n",
    "        avg_acc_all_models[model_type][baseline_type] = []\n",
    "        avg_f1_all_models[model_type][baseline_type] = []\n",
    "\n",
    "    avg_acc_all_models[model_type][baseline_type].append((top_n, accuracy_t))\n",
    "    avg_f1_all_models[model_type][baseline_type].append((top_n, f1_score_t))\n",
    "        \n",
    "for model_type in avg_acc_all_models:\n",
    "    for baseline_type in avg_acc_all_models[model_type]:\n",
    "        avg_acc_all_models[model_type][baseline_type].sort(key=lambda x: x[0])\n",
    "        avg_f1_all_models[model_type][baseline_type].sort(key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T13:26:16.791773Z",
     "iopub.status.idle": "2025-02-09T13:26:16.792170Z",
     "shell.execute_reply": "2025-02-09T13:26:16.791982Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_metrics(metrics, metric_name):\n",
    "    folder_save_fig = f\"/kaggle/working/cmp\"\n",
    "    if not os.path.exists(folder_save_fig):\n",
    "        os.makedirs(folder_save_fig)\n",
    "    path_save_fig = f'{folder_save_fig}/{metric_name}.png'\n",
    "    plt.figure(figsize = (8,4), dpi=300)\n",
    "    for baseline_name, values in metrics.items():\n",
    "        top_n_list = [item[0] for item in values]\n",
    "        metric_values = [item[1] for item in values]\n",
    "        plt.plot(top_n_list, metric_values, marker='o', label=baseline_name)\n",
    "    plt.title(f'{metric_name} for different baselines and top N features')\n",
    "    plt.xlabel('Top N features')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend(fontsize='small', bbox_to_anchor=(1.05, 1), loc='upper left')    \n",
    "    plt.xticks(top_n_list)\n",
    "    plt.yticks(list(range(35, 91, 5)))\n",
    "    plt.grid(axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path_save_fig, facecolor='white', edgecolor='white')\n",
    "    plt.show()\n",
    "    \n",
    "# Plot accuracy\n",
    "for model_type in avg_acc_all_models.keys():\n",
    "    print('*' * 40, model_type, '*' * 40)\n",
    "    plot_metrics(avg_acc_all_models[model_type], f'Accuracy for {model_type}')\n",
    "    plot_metrics(avg_f1_all_models[model_type], f'F1 Score for {model_type}')\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T13:26:16.793034Z",
     "iopub.status.idle": "2025-02-09T13:26:16.793363Z",
     "shell.execute_reply": "2025-02-09T13:26:16.793210Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convenient for modifying plot\n",
    "json_file = \"/kaggle/working/results.json\"\n",
    "\n",
    "with open(json_file, mode='w', encoding='utf-8') as file:\n",
    "    json.dump(filtered_results, file, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5211395,
     "sourceId": 10408010,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30097,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
